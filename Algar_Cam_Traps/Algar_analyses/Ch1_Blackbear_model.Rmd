---
title: "Ch1_models_Blackbear"
author: "Erin Tattersall"
date: "May 13, 2018"
output: word_document
---

  Based on model selection comparison of underlying distributions and zero-inflation, I chose an nbinom1 distribution for Blackbear data, with zero-inflation and ActiveDays in the ZI model (see Ch1_Blackbear_modelDistribution.Rmd)    
Here I will: 
1. Double check random structure using all covariates
2. Build models assessing Treatment effect, including other combinations of covariates to account for additional noise and compare their effect to Treatment 
4. Perform model selection with AIC  
5. Calculate evidence ratios (AICwt of Best Model/ AICwt of other models)  
6. Checking residuals of Top Model
  
Previous scale analysis showed lowland habitat at 500m and linear density measured at 750m best explained Blackbear detections


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(glmmTMB)
library(bbmle) #AICtab function
library(ggplot2)
library(lmtest)
library(reshape)
library(dplyr)
library(knitr)
library(MuMIn) #streamlining model selection

```
```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
#Load detection data
det <- read.csv("Seismic_nov2015-apr2018.csv")

#Load lowland data
low <- read.csv("newAVIE_lowland8buffersizes.csv")

#Combine data
det$low250 <- low$low250[match(det$Site, low$CamStation)]
det$low500 <- low$low500[match(det$Site, low$CamStation)]
det$low750 <- low$low750[match(det$Site, low$CamStation)]
det$low1000 <- low$low1000[match(det$Site, low$CamStation)]
det$low1250 <- low$low1250[match(det$Site, low$CamStation)]
det$low1500 <- low$low1500[match(det$Site, low$CamStation)]
det$low1750 <- low$low1750[match(det$Site, low$CamStation)]
det$low2000 <- low$low2000[match(det$Site, low$CamStation)]

#Load linedensity data
LineDens <- read.csv("AlgarStationsLD_Lines.csv")

# Combine datasets
det$LD250 <- LineDens$X250m[match(det$Site, LineDens$CamStation)]
det$LD500 <- LineDens$X500m[match(det$Site, LineDens$CamStation)]
det$LD750 <- LineDens$X750m[match(det$Site, LineDens$CamStation)]
det$LD1000 <- LineDens$X1000m[match(det$Site, LineDens$CamStation)]
det$LD1250 <- LineDens$X1250m[match(det$Site, LineDens$CamStation)]
det$LD1500 <- LineDens$X1500m[match(det$Site, LineDens$CamStation)]
det$LD1750 <- LineDens$X1750m[match(det$Site, LineDens$CamStation)]
det$LD2000 <- LineDens$X2000m[match(det$Site, LineDens$CamStation)]


#Load line width and veg height
sp <- read.csv("Spatial_covariates.csv")

det$VegHt <- sp$VegHt[match(det$Site, sp$Site)]
det$LineWidth <- sp$LineWidth[match(det$Site, sp$Site)]


### Centring and standardizing covariates
## All continuous input variables need to be centred and standardized ((value - mean)/2SD; Gelman, 2008)
covscale <- function(covariate){
  centre <- (covariate - mean(covariate, na.rm = TRUE)) # where covariate is a vector of numeric input values for one covariate
  standardize <- centre/(2*sd(covariate, na.rm = TRUE)) # Divide centred value by 2 SD
}

summary(det$ActiveDays) #mean 18
sd(det$ActiveDays)
det$ActiveDays_sc <- covscale(det$ActiveDays)
summary(det$ActiveDays_sc)
sd(det$ActiveDays_sc)
det$D2W_sc <- covscale(det$Dist2Water_km) #mean 2.07
summary(det$Dist2Water_km) #mean 2.07
sd(det$Dist2Water_km) #1.76
summary(det$D2W_sc) #mean 0
sd(det$D2W_sc) #SD

det$pSnow_sc <- covscale(det$pSnow)
summary(det$pSnow_sc) #NAs, probably because of NAs in pSnow --> added na.rm = TRUE to function
det$pSnow_sc <- covscale(det$pSnow)
summary(det$pSnow_sc)
sd(det$pSnow_sc, na.rm = TRUE)
det$low250_sc <- covscale(det$low250)
det$low500_sc <- covscale(det$low500)
det$low750_sc <- covscale(det$low750)
det$low1000_sc <- covscale(det$low1000)
det$low1250_sc <- covscale(det$low1250)
det$low1500_sc <- covscale(det$low1500)
det$low1750_sc <- covscale(det$low1750)
det$low2000_sc <- covscale(det$low2000)
det$LD250_sc <- covscale(det$LD250)
det$LD500_sc <- covscale(det$LD500)
det$LD750_sc <- covscale(det$LD750)
det$LD1000_sc <- covscale(det$LD1000)
det$LD1250_sc <- covscale(det$LD1250)
det$LD1500_sc <- covscale(det$LD1500)
det$LD1750_sc <- covscale(det$LD1750)
det$LD2000_sc <- covscale(det$LD2000)
det$LineWidth_sc <- covscale(det$LineWidth)
det$VegHt_sc <- covscale(det$VegHt)

summary(det[ , 36:56]) #Centred means of 0

write.csv(det, "Seismic_nov2015-apr2018.csv")

#Filter for desired months and covariates - low500 and LD750
det <- det %>% filter(Yr_Month == "2016-04" | Yr_Month == "2016-05" | Yr_Month == "2016-06"| Yr_Month == "2016-07"| Yr_Month == "2016-08"| Yr_Month == "2016-09"| Yr_Month == "2016-10"| Yr_Month == "2017-04" | Yr_Month == "2017-04" | Yr_Month == "2017-05" | Yr_Month == "2017-06"| Yr_Month == "2017-07"| Yr_Month == "2017-08"| Yr_Month == "2017-09"| Yr_Month == "2017-10"| Yr_Month == "2018-04" | Yr_Month == "2018-05" | Yr_Month == "2018-06"| Yr_Month == "2018-07"| Yr_Month == "2018-08"| Yr_Month == "2018-09"| Yr_Month == "2018-10") %>% select(Site, Treatment,Yr_Month, Site_ym, Blackbear, Month, ActiveDays_sc, pSnow_sc,low500_sc, LD750_sc, VegHt_sc, LineWidth_sc)
#Omit NA rows
det <- na.omit(det)
```
### 1. Random structure and Active Days 
  Random structure was previously assessed, but here I will confirm using all model covariates. Models have convergence problems when ActiveDays is run in the zero-inflated part of the model, so here I include it in the conditional

```{r, echo=FALSE, message=FALSE}
r0 <- glmmTMB(Blackbear~ LineWidth_sc + LD750_sc + VegHt_sc + low500_sc + Treatment + ActiveDays_sc, data = det, zi=~1, family = nbinom2)
rSite <- glmmTMB(Blackbear~ LineWidth_sc + LD750_sc + VegHt_sc + low500_sc + Treatment + ActiveDays_sc + (1|Site), data = det, zi=~1,  family = nbinom2) 
rMonth <- glmmTMB(Blackbear~ LineWidth_sc + LD750_sc + VegHt_sc + low500_sc + Treatment + ActiveDays_sc + (1|Month), data = det, zi=~1, family = nbinom2)
r2 <- glmmTMB(Blackbear~ LineWidth_sc + LD750_sc + VegHt_sc + low500_sc + Treatment + ActiveDays_sc + (1|Site) + (1|Month), data = det, zi=~1, family = nbinom2)

ICtab(r0, rSite, rMonth, r2, type= "AIC", weights = TRUE, delta = TRUE, logLik = TRUE, sort=TRUE)
```  
   Continue modelling with 2 random effects.
## Model Set  
  (note that numbered models from dredge do not correspond with numbers in table; I have listed models in order of increasing complexity, dredge did not)
  Also, Blackbear model sets do not include Snow, as they are active in the Snow-free period

```{r, echo=FALSE, message=FALSE, results = 'hide'}
# dredge function from MuMin generates and evaluates model sets
global.model <- glmmTMB(Blackbear~ LineWidth_sc + LD750_sc + VegHt_sc + low500_sc + Treatment + ActiveDays_sc + (1|Site) + (1|Month), data = det, zi=~1, family = nbinom2)

#List of all possible models including Treatment and some combination of other covariates. Runs models and returns a model selection data frame
Treat.models <- dredge(global.model = global.model, beta = "none", evaluate = TRUE, rank = "AIC", fixed = ~cond(Treatment) + cond(ActiveDays_sc), trace = TRUE) # Does not include null model

#Lists model objects
Models <- get.models(Treat.models, subset = TRUE)

## Add NULL model to list
NULLmod <- glmmTMB(Blackbear~ ActiveDays_sc + (1|Site) + (1|Month), data = det, zi=~1, family = nbinom2)
Models[["Nullmod"]] <- NULLmod

```

```{r, echo=FALSE}
BlackbearAIC <- ICtab(Models, mnames = names(Models), type= "AIC", weights = TRUE, delta = TRUE, logLik = TRUE, sort=TRUE)
BlackbearAIC
``` 
```{r, echo=FALSE, results='hide'}
summary(Models$`13`)
summary(Models$`9`)

```
  Three models within 2 dAIC points of each other, with model weights between 13-25%. Top model is the Null, with only ActiveDays as a covariate
    
  
## Evidence Ratios and Cumulative model weight (calculating confidence intervals)
  
  Calculating evidence ratios (AIC wt of best model/AIC weight of others) gives:
  
```{r,echo=FALSE, results='hide'}
EvR <- function(AICwt.high, AICwt.low){
  EvR <- AICwt.high/AICwt.low
  print(EvR)
}
EvR(BlackbearAIC$weight[1], BlackbearAIC$weight[2])
BlackbearWt <- BlackbearAIC$weight
BlackbearER <- vector(length=16)
for(i in 2:17){
  BlackbearER[i-1] <- EvR(BlackbearWt[1], BlackbearWt[i])
}
```
```{r,echo=FALSE}
BlackbearER <- append("", BlackbearER)
ER <- as.data.frame(c("Nullmod", names(Models[1:16]))) # Order needed to be corrected as Nullmod should be at top
colnames(ER) <- "ModelNames"
ER$dLogLikelihood <- BlackbearAIC$dLogLik
ER$dAIC <- BlackbearAIC$dAIC
ER$Modelweight <- BlackbearAIC$weight
ER$CumulativeWeight <- rep(NA,17)
ER$CumulativeWeight[1] <- ER$Modelweight[1]
for (i in 2:length(ER$Modelweight)){
  ER$CumulativeWeight[i] <- ER$Modelweight[i] + ER$CumulativeWeight[i-1]
}
ER$EvidenceRatio <- BlackbearER

ER
```
 Examining summaries for top 2 models that aren't the null (2dAIC)  
```{r, echo=FALSE, results='hide'}
fixef(Models$`9`)
fixef(Models$`13`)

```  
In addition to ActiveDays and Treatment, both include VegHt and second includes low


```{r, echo=FALSE}


## Summary for full model
full.Blackbear <- glmmTMB(Blackbear ~  Treatment + low500_sc + LineWidth_sc + LD750_sc + VegHt_sc + ActiveDays_sc + (1|Site) + (1|Month), data = det, zi=~1, family = nbinom1)
summary(full.Blackbear)

```
### Model averaging (NOT DONE FOR 36MONTH DATA)
  Two models are within 2dAIC scores of  each other, suggesting that they all explain the data equally well. As my goal is to compare Treatment effects to the effects of other covariates, I do not just want the estimates given in the top model, but rather the best possible estimates for many covariates. I will therefore model average to obtain a weighted average estimate (effect size) of covariates included in models that are within 2 dAIC of one another or within 95% confidence intervals, whichever is more conservative.
```{r, echo=FALSE}
#List of top models
Beartop <- Models[c(1,2,17)]
Bear.averaged <- model.avg(Beartop)
summary(Bear.averaged) ## Look at full model --> parameters averaged over all models, with the coefficient set to 0 if the parameter is not present
Est.full <- Bear.averaged$coefficients[1,] #Estimates for the full average
summary(Bear.averaged[3])
CI95.full <- confint(Bear.averaged)
BearEffects.CI <- as.data.frame(c("Intercept", "ActiveDays",  "ziIntercept", "VegHt", "HumanUse", "NatRegen", "SPP", "Lowland"))
colnames(BearEffects.CI) <- "Predictor"
BearEffects.CI$Estimate <- Est.full
BearEffects.CI$CIlow <- CI95.full[, 1]
BearEffects.CI$CI.high <- CI95.full[, 2]
```

### Predictor Effect Sizes
```{r, echo=FALSE}
library(ggplot2)
ggplot(data = BearEffects.CI, aes( x = Predictor, y = Estimate)) + geom_point() + geom_errorbar(aes(ymin=CIlow, ymax = CI.high))+ geom_hline(yintercept = 0) + scale_x_discrete(limits=c("NatRegen", "SPP", "HumanUse", "Lowland", "VegHt"))+theme_classic() + theme(axis.text.x = element_text(angle = 45, hjust = 1, colour = "black", size = 12)) + theme(axis.title.x = element_text(angle = 0, colour = "black", size = 14)) + theme(axis.title.y = element_text(angle = 90, colour = "black", size = 14)) + theme(strip.text = element_text(colour = "black", size = 14)) + scale_y_continuous(limits = c(-3, 3))
```